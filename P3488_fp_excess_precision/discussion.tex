\section{Discussion}

A general observation:
A simplification where the implementation were free to use excess precision at
runtime as it deems best would lead to suprising results:
Consider two floating-point values \code{a} and \code{b} where
\code{std::isfinite(b)} is statically known to be \code{true}.
With arbitrary excess precision the optimizer would then be allowed to replace
\code{a + b - b} with \code{a}.

A general consequence of excess precision is that \fp evaluation leads to
double rounding and thus potentially worse errors.
However, for \code{+-*/} and \code{sqrt}, if there are twice as many precision
bits in the intermediate type, then double rounding after each operation does
not lead to additional errors.
Where the second rounding occurs is not fully reproducible and can potentially
change via unrelated code changes in the translation unit\footnote{e.g. because of register allocation}.

Without excess precision \code{std::float16_t} and \code{std::bfloat16_t} can
either use a soft-float implementation, round back after every \code{float32_t}
operation, or dedicated hardware is required.
Using \float (binary32) instructions without rounding back down after every
operation is impossible with the current possible values for
\code{FLT_EVAL_METHOD}.
An implementation that wants to evaluate \std\code{float16_t} /
\std\code{bfloat16_t} in higher intermediate precision needs to set
\code{FLT_EVAL_METHOD} to 1 or 2 (or 32?).

\subsection{strictest: Disallow all excess precision}\label{d:1}

I believe [expr.pre] p6 is fairly clear that it was never the design intent to
exclude all excess precision.

Implications of disallowing all excess precision:
\begin{itemize}
  \item The x87 FPU cannot be used with a single “precision control” value,
    because double rounding is not correct (e.g. FPU configured to 80-bit with
    subsequent rounding to 64/32-bit).
    This implies that the compiler would have to set the x87 floating-point
    control word (FPCW) using the FLDCW instruction whenever it needs to
    execute \fp operations (with different precision).

  \item However, the x87 FPU is not really an important target anymore. Every x86
    CPU since the last 20 years can use SSE instructions instead.
\end{itemize}

\subsection{compatible: Do exactly the same as C}\label{d:2}

It might have been the original intent to do the same as C, but [lex.fcon] p3
suggests otherwise.

Implications of adopting this as resolution:
\begin{itemize}
  \item \code{x + 3.14f;} can require 8, 12, 16, or even more bytes to store
    the constant in the resulting binary.
    (This is the status quo of GCC since version 13.)

  \item \code{float x = 3.14f; assert(x == 3.14f);} is allowed to fail
    depending on implementation, target, and compiler flags.
    (This is the status quo of GCC since version 13.)
\end{itemize}

\subsection{like C but only for runtime evaluation}\label{d:3}

\begin{itemize}
  \item The intent here appears to be that we want to prescribe reproducible
    \fp behavior.

  \item However, since that has potentially dramatic consequences on runtime
    performance, this restriction is only a recomendation for runtime
    evaluation.
    We thus acknowledge the existence of hardware where reproducible \fp
    behavior comes at unreasonable performance cost.
    Because of these cases --- and only for these --- [expr.pre] allows excess
    precision in evaluation, which should be reflected by non-zero
    \code{FLT_EVAL_METHOD}.

  \item We should consider a new type trait along the lines of
    \begin{lstlisting}
template <floating-point T>
struct evaluation_type {
  using type = @\seebelow@;
};

template <floating-point T>
using evaluation_type_t = typename evaluation_type<T>::type;
    \end{lstlisting}
    Where e.g. \code{evaluation_type_t<float16_t>} could be \code{float}.
    This would supersede the use of the \code{FLT_EVAL_METHOD}.
    Implementations could then reasonably set \code{FLT_EVAL_METHOD} to
    \code{-1} and rely solely on the traits for reflection of \fp evaluation
    behavior.
\end{itemize}
